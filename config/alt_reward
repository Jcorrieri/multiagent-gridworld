environment:
    env_name: alt_reward
    base_station: False
    fov: 4
    max_steps: 1000
    size: 25
    num_agents: 4
    cr: 10

reward_scheme:
    new_tile_visited: 2.0
    old_tile_maintainer: 0.5
    old_tile_stagnant: -0.1
    disconnected: -2.0
    obstacle_penalty: -0.5
    termination_bonus: 200

training:
    module_file: cnn_3conv2linear.py
    num_episodes: 10000
    target_reward: 10000
    gamma: 0.95
    lr: 0.0001
    grad_clip: 1.0
    train_batch_size: 8000
    num_passes: 5
    minibatch_size: 800
    l2_regularization: 0.0001
    lambda_: 0.90
    entropy_coeff: [[0, 0.01]]
    #clip_param: 0.3

testing:
    num_episodes_per_map: 1
    seed: 42
    explore: True
    render: True
    model_path: default-env/v1
    checkpoint: -1