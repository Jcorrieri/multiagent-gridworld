environment:
    env_name: alt_reward
    base_station: False
    fov: 4
    max_steps: 1600
    size: 25
    num_agents: 6
    cr: 10

reward_scheme:
    new_tile_visited: 1.0
    old_tile_maintainer: 0.5
    old_tile_stagnant: -0.1
    disconnected: -2.0
    obstacle_penalty: -0.2
    termination_bonus: 100

training:
    module_file: cnn_2conv2linear.py
    num_episodes: 6000
    target_reward: 2500
    gamma: 0.90
    lr: 0.0001
    grad_clip: 1.0
    train_batch_size: 8000
    num_passes: 5
    minibatch_size: 800
    l2_regularization: 0.0001
    lambda_: 0.90
    entropy_coeff: [[0, 0.01]]
    #clip_param: 0.3

testing:
    num_episodes_per_map: 1
    seed: 42
    explore: True
    render: True
    model_path: default-env/v0
    checkpoint: 1